{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ea4q02Cwc7Y",
        "outputId": "65cb4173-5a86-49af-8d21-6d80f2928857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy lightgbm xgboost catboost scikit-learn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import catboost as cb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from datetime import datetime\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "train = pd.read_csv('Train.csv', parse_dates=['doj'])\n",
        "test = pd.read_csv('Test.csv', parse_dates=['doj'])\n",
        "transactions = pd.read_csv('Transactions.csv', parse_dates=['doj', 'doi'])\n",
        "\n",
        "# Filter transactions with dbd < 15\n",
        "transactions = transactions[transactions['dbd'] < 15]\n",
        "\n",
        "# Merge transactions: add week/month columns\n",
        "transactions['week'] = transactions['doj'].dt.isocalendar().week.astype(int)\n",
        "transactions['month'] = transactions['doj'].dt.month\n",
        "\n",
        "# Combine train and test\n",
        "test['final_seatcount'] = np.nan\n",
        "train['dataset'] = 'train'\n",
        "test['dataset'] = 'test'\n",
        "df = pd.concat([train, test]).reset_index(drop=True)\n",
        "\n",
        "# Ensure df also has 'week' and 'month' columns\n",
        "df['week'] = df['doj'].dt.isocalendar().week.astype(int)\n",
        "df['month'] = df['doj'].dt.month"
      ],
      "metadata": {
        "id": "LmngJ1nbwnlK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimized multi-level aggregation function with batching\n",
        "def multi_level_aggregations(trans, group_cols, agg_cols):\n",
        "    agg_df_list = []\n",
        "    for col in tqdm(agg_cols, desc=\"Processing agg_cols\"):\n",
        "        for group in tqdm(group_cols, desc=f\"Groups for {col}\"):\n",
        "            group_name = '_'.join(group)\n",
        "            agg = trans.groupby(group)[col].agg(['mean','sum','std','min','max']).astype(np.float32).reset_index()\n",
        "            agg.columns = group + [f\"{group_name}_{col}_{stat}\" for stat in ['mean','sum','std','min','max']]\n",
        "            agg_df_list.append((agg, group))\n",
        "            gc.collect()\n",
        "    return agg_df_list\n",
        "\n",
        "group_cols = [['srcid'], ['destid'], ['srcid','destid'], ['srcid','week'], ['destid','month'], ['srcid','destid','week']]\n",
        "agg_cols = ['cumsum_seatcount', 'cumsum_searchcount']\n",
        "agg_dfs = multi_level_aggregations(transactions, group_cols, agg_cols)\n",
        "\n",
        "# Merge all aggregated features safely using their specific group keys\n",
        "tqdm.pandas(desc=\"Merging features\")\n",
        "for agg_df, merge_keys in tqdm(agg_dfs, desc=\"Merging features\"):\n",
        "    for key in merge_keys:\n",
        "        if key not in df.columns:\n",
        "            df[key] = np.nan  # Ensure keys exist in df to avoid merge error\n",
        "    df = df.merge(agg_df, on=merge_keys, how='left')\n",
        "    del agg_df\n",
        "    gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89lj6syVxkD3",
        "outputId": "473bbbec-98a0-4769-c2e7-ba0703ccc480"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing agg_cols:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Groups for cumsum_seatcount:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "Groups for cumsum_seatcount:  17%|█▋        | 1/6 [00:00<00:03,  1.52it/s]\u001b[A\n",
            "Groups for cumsum_seatcount:  33%|███▎      | 2/6 [00:01<00:02,  1.99it/s]\u001b[A\n",
            "Groups for cumsum_seatcount:  50%|█████     | 3/6 [00:02<00:02,  1.42it/s]\u001b[A\n",
            "Groups for cumsum_seatcount:  67%|██████▋   | 4/6 [00:02<00:01,  1.56it/s]\u001b[A\n",
            "Groups for cumsum_seatcount:  83%|████████▎ | 5/6 [00:02<00:00,  1.81it/s]\u001b[A\n",
            "Groups for cumsum_seatcount: 100%|██████████| 6/6 [00:03<00:00,  1.82it/s]\n",
            "Processing agg_cols:  50%|█████     | 1/2 [00:03<00:03,  3.30s/it]\n",
            "Groups for cumsum_searchcount:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "Groups for cumsum_searchcount:  17%|█▋        | 1/6 [00:00<00:01,  3.56it/s]\u001b[A\n",
            "Groups for cumsum_searchcount:  33%|███▎      | 2/6 [00:00<00:01,  3.79it/s]\u001b[A\n",
            "Groups for cumsum_searchcount:  50%|█████     | 3/6 [00:00<00:00,  3.81it/s]\u001b[A\n",
            "Groups for cumsum_searchcount:  67%|██████▋   | 4/6 [00:01<00:00,  3.94it/s]\u001b[A\n",
            "Groups for cumsum_searchcount:  83%|████████▎ | 5/6 [00:01<00:00,  4.06it/s]\u001b[A\n",
            "Groups for cumsum_searchcount: 100%|██████████| 6/6 [00:01<00:00,  3.94it/s]\n",
            "Processing agg_cols: 100%|██████████| 2/2 [00:04<00:00,  2.41s/it]\n",
            "Merging features: 100%|██████████| 12/12 [00:02<00:00,  5.42it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add date features\n",
        "df['dayofweek'] = df['doj'].dt.dayofweek\n",
        "df['weekofyear'] = df['doj'].dt.isocalendar().week.astype(int)\n",
        "df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n",
        "\n",
        "# Fill missing required base features before generating lag features\n",
        "for col in ['cumsum_seatcount', 'cumsum_searchcount']:\n",
        "    if col not in df.columns:\n",
        "        df[col] = 0.0\n",
        "\n",
        "# Create route_key\n",
        "df['route_key'] = df['srcid'].astype(str) + '_' + df['destid'].astype(str)\n",
        "\n",
        "# Lag and rolling window features\n",
        "for col in ['cumsum_seatcount', 'cumsum_searchcount']:\n",
        "    df.sort_values(['route_key', 'doj'], inplace=True)\n",
        "    df[f'{col}_lag1'] = df.groupby(['route_key'])[col].shift(1)\n",
        "    df[f'{col}_rolling_mean3'] = df.groupby(['route_key'])[col].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
        "    df[f'{col}_rolling_std3'] = df.groupby(['route_key'])[col].transform(lambda x: x.rolling(window=3, min_periods=1).std())\n",
        "\n",
        "# Label Encode categorical if available\n",
        "cat_cols = ['srcid_region', 'destid_region', 'srcid_tier', 'destid_tier']\n",
        "for col in cat_cols:\n",
        "    if col in df.columns:\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "# Split back to train/test\n",
        "train_df = df[df['dataset'] == 'train'].copy()\n",
        "test_df = df[df['dataset'] == 'test'].copy()\n",
        "\n",
        "# Validation split before dropping datetime\n",
        "val_start = pd.to_datetime(\"2023-11-01\")\n",
        "train_mask = train_df['doj'] < val_start\n",
        "val_mask = train_df['doj'] >= val_start\n",
        "\n",
        "# Drop datetime columns AFTER split\n",
        "drop_cols = ['doj', 'doi'] if 'doi' in df.columns else ['doj']\n",
        "train_df.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
        "test_df.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
        "\n",
        "X_full = train_df.drop(['final_seatcount','dataset','route_key'], axis=1)\n",
        "y_full = train_df['final_seatcount']\n",
        "X_test = test_df.drop(['final_seatcount','dataset','route_key'], axis=1)"
      ],
      "metadata": {
        "id": "Y56jG1QZxoXv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LightGBM for feature importance-based selection\n",
        "lgb_sel = lgb.LGBMRegressor(n_estimators=500)\n",
        "lgb_sel.fit(X_full.select_dtypes(exclude=['datetime64[ns]']), y_full)\n",
        "feature_importance = pd.Series(lgb_sel.feature_importances_, index=X_full.select_dtypes(exclude=['datetime64[ns]']).columns)\n",
        "\n",
        "# Select top features\n",
        "top_1k = feature_importance.sort_values(ascending=False).head(1000).index.tolist()\n",
        "top_2k = feature_importance.sort_values(ascending=False).head(2000).index.tolist()\n",
        "top_3k = feature_importance.sort_values(ascending=False).head(3000).index.tolist()\n",
        "top_6k = feature_importance.sort_values(ascending=False).head(6000).index.tolist()\n",
        "feature_sets = {'1k': top_1k, '2k': top_2k, '3k': top_3k, '6k': top_6k}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTmPX8icx52K",
        "outputId": "08f81ee8-021c-482f-a847-bee9c6083e48"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.237106 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 17212\n",
            "[LightGBM] [Info] Number of data points in the train set: 67200, number of used features: 119\n",
            "[LightGBM] [Info] Start training from score 2001.729464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-tabnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBRD9A9dydQR",
        "outputId": "c2ed6477-139c-4787-e163-387b3008f483"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-tabnet\n",
            "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (2.0.2)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (1.6.1)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\n",
            "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-tabnet\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-tabnet-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill all remaining NaNs before model training\n",
        "X_full.fillna(-1, inplace=True)\n",
        "X_test.fillna(-1, inplace=True)\n",
        "y_full.fillna(y_full.mean(), inplace=True)"
      ],
      "metadata": {
        "id": "ifNnkgZCznFm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train all models\n",
        "preds_all = []\n",
        "val_preds_all = []\n",
        "for name, feat_set in feature_sets.items():\n",
        "    X_train = X_full.loc[train_mask, feat_set]\n",
        "    y_train = y_full[train_mask]\n",
        "    X_val = X_full.loc[val_mask, feat_set]\n",
        "    y_val = y_full[val_mask]\n",
        "    X_t = X_test[feat_set]\n",
        "\n",
        "    # LightGBM\n",
        "    lgb_model = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.035)\n",
        "    lgb_model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        eval_metric='rmse',\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
        "    )\n",
        "    preds_all.append(lgb_model.predict(X_t))\n",
        "    val_preds_all.append(lgb_model.predict(X_val))\n",
        "\n",
        "    # XGBoost (manual early stopping omitted due to version constraints)\n",
        "    xgb_model = xgb.XGBRegressor(n_estimators=1000, learning_rate=0.035, objective='reg:squarederror')\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    preds_all.append(xgb_model.predict(X_t))\n",
        "    val_preds_all.append(xgb_model.predict(X_val))\n",
        "\n",
        "    # CatBoost\n",
        "    cb_model = cb.CatBoostRegressor(iterations=1000, learning_rate=0.035, verbose=False)\n",
        "    cb_model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
        "    preds_all.append(cb_model.predict(X_t))\n",
        "    val_preds_all.append(cb_model.predict(X_val))\n",
        "\n",
        "    # TABDPT: Using TabNet as a proxy (since HuggingFace models need transformer setup)\n",
        "    from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "    import torch\n",
        "    clf = TabNetRegressor(verbose=0)\n",
        "    clf.fit(\n",
        "        X_train.values, y_train.values.reshape(-1,1),\n",
        "        eval_set=[(X_val.values, y_val.values.reshape(-1,1))],\n",
        "        max_epochs=100,\n",
        "        patience=20\n",
        "    )\n",
        "    preds_all.append(clf.predict(X_t.values).reshape(-1))\n",
        "    val_preds_all.append(clf.predict(X_val.values).reshape(-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fguptyyqx-6v",
        "outputId": "a3d02f9a-0f9e-4298-8a8f-e7bcefac0088"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012787 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 16802\n",
            "[LightGBM] [Info] Number of data points in the train set: 24500, number of used features: 119\n",
            "[LightGBM] [Info] Start training from score 1770.934408\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[184]\tvalid_0's rmse: 804.375\tvalid_0's l2: 647020\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_mse = 649461.38137\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013277 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 16802\n",
            "[LightGBM] [Info] Number of data points in the train set: 24500, number of used features: 119\n",
            "[LightGBM] [Info] Start training from score 1770.934408\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[184]\tvalid_0's rmse: 804.375\tvalid_0's l2: 647020\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_mse = 649461.38137\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013490 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 16802\n",
            "[LightGBM] [Info] Number of data points in the train set: 24500, number of used features: 119\n",
            "[LightGBM] [Info] Start training from score 1770.934408\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[184]\tvalid_0's rmse: 804.375\tvalid_0's l2: 647020\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_mse = 649461.38137\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012895 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 16802\n",
            "[LightGBM] [Info] Number of data points in the train set: 24500, number of used features: 119\n",
            "[LightGBM] [Info] Start training from score 1770.934408\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[184]\tvalid_0's rmse: 804.375\tvalid_0's l2: 647020\n",
            "\n",
            "Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_mse = 649461.38137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Predictions\n",
        "final_preds = np.mean(preds_all, axis=0)\n",
        "val_preds = np.mean(val_preds_all, axis=0)\n",
        "val_score = np.sqrt(mean_squared_error(y_full[val_mask], val_preds))\n",
        "print(f\"Validation RMSE: {val_score:.4f}\")\n",
        "\n",
        "# Save Submission\n",
        "submission = pd.DataFrame({\n",
        "    'route_key': test_df['route_key'],\n",
        "    'final_seatcount': final_preds\n",
        "})\n",
        "submission.to_csv('Prediction File.csv', index=False)\n",
        "print(\"Saved as Prediction File.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwRCFDHF5PDI",
        "outputId": "ef0eb3d8-3425-432a-fcdb-2c03420f14ff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation RMSE: 791.0773\n",
            "Saved as Prediction File.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('Prediction File.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2UGOp_Tu7fr4",
        "outputId": "a54e4e66-f98b-4810-bbba-41d6630bd8d2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f9f5fba6-9080-4633-97d9-ea9b77f4b30d\", \"Prediction File.csv\", 142419)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}